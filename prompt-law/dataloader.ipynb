{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tiktoken\n",
    "from langchain_community.document_loaders import DataFrameLoader\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.vectorstores import faiss\n",
    "from langchain.embeddings import CacheBackedEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.storage import LocalFileStore\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.runnable import RunnablePassthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/crysis/anaconda3/envs/torch/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, LlamaForCausalLM\n",
    "from peft import PeftModel, PeftConfig\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import dotenv_values\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "import os\n",
    "\n",
    "env_vars = dotenv_values('../.env')\n",
    "os.environ['OPENAI_API_KEY'] = env_vars.get('OPENAI_API_KEY')\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = env_vars.get('LANGCHAIN_TRACING_V2')\n",
    "os.environ['LANGCHAIN_ENDPOINT'] = env_vars.get('LANGCHAIN_ENDPOINT')\n",
    "os.environ['LANGCHAIN_API_KEY'] = env_vars.get('LANGCHAIN_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instruction</th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>동거 중인데 갑자기 헤어지자는 통보를 받았어요. 사실혼인 경우에도 위자료를 받을 수...</td>\n",
       "      <td>위자료를 받을 수 있습니다. 사실혼은 부부간 합의 또는 부부 어느 한 쪽의 일방적인...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>이혼이 취소될 수 있나요?</td>\n",
       "      <td>재판상 이혼은 재판절차를 거쳐 이혼판결이 선고된 것이므로 취소될 수 없지만, 협의이...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>이혼해도 자녀를 만날 수 있나요?</td>\n",
       "      <td>이혼 후 자녀를 직접 양육하지 않는 부모 일방과 자녀는 서로 만나거나 연락할 수 있...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>이혼한 후에 자녀의 성과 본을 저의 성과 본으로 바꿀 수 있나요?</td>\n",
       "      <td>이혼 후 자녀의 성과 본을 자신의 성과 본으로 바꿀 수 있습니다. ◇ 법원 허가 자...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>중학생 딸아이가 학교 숙제로 유언장을 작성했는데, 이 유언장이 법적으로 효력 있는 ...</td>\n",
       "      <td>유언은 의사능력이 있는 17세(유언 적령)에 달한 사람이 할 수 있습니다. 따라서 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2270</th>\n",
       "      <td>시청에 정기적으로 문구류를 납품하는 수의계약에 참여했는데, 수의계약의 계약상대자는 ...</td>\n",
       "      <td>수의계약대상자는 견적제출자의 견적가격과 계약이행능력 등에 따라 결정되며, 원칙적으로...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2271</th>\n",
       "      <td>물품계약을 체결한 후 물가가 급격히 올라서 계약 당시의 금액으로는 수량을 맞추기 어...</td>\n",
       "      <td>아니요. 물품계약을 체결한 날부터 90일 이상 지난 후 입찰일을 기준일로 하여 품목...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2272</th>\n",
       "      <td>계약을 체결한 후에는 계약 완료 전이라도 대금을 미리 받을 수 있다고 하던데, 얼마...</td>\n",
       "      <td>계약을 이행하기 전이라도 일정 요건에 해당하면 계약금액의 100분의 70을 초과하지...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2273</th>\n",
       "      <td>물품을 납품하기 직전에 천재지변 등 불가항력의 사유로 대형화재가 발생하여 해당 물품...</td>\n",
       "      <td>계약상대자의 책임 없이 이행이 지체되는 경우에는 계약기간 연장신청을 할 수 있고, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2274</th>\n",
       "      <td>물품계약 입찰의 참가자격이 안된다고 해서 이의신청을 했고 결과를 받았는데, 그 결과...</td>\n",
       "      <td>아니요. 이의신청에 따른 조치에 대하여 이의가 있으면 재심(再審)을 청구할 수 있습...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2275 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            instruction  \\\n",
       "0     동거 중인데 갑자기 헤어지자는 통보를 받았어요. 사실혼인 경우에도 위자료를 받을 수...   \n",
       "1                                        이혼이 취소될 수 있나요?   \n",
       "2                                    이혼해도 자녀를 만날 수 있나요?   \n",
       "3                  이혼한 후에 자녀의 성과 본을 저의 성과 본으로 바꿀 수 있나요?   \n",
       "4     중학생 딸아이가 학교 숙제로 유언장을 작성했는데, 이 유언장이 법적으로 효력 있는 ...   \n",
       "...                                                 ...   \n",
       "2270  시청에 정기적으로 문구류를 납품하는 수의계약에 참여했는데, 수의계약의 계약상대자는 ...   \n",
       "2271  물품계약을 체결한 후 물가가 급격히 올라서 계약 당시의 금액으로는 수량을 맞추기 어...   \n",
       "2272  계약을 체결한 후에는 계약 완료 전이라도 대금을 미리 받을 수 있다고 하던데, 얼마...   \n",
       "2273  물품을 납품하기 직전에 천재지변 등 불가항력의 사유로 대형화재가 발생하여 해당 물품...   \n",
       "2274  물품계약 입찰의 참가자격이 안된다고 해서 이의신청을 했고 결과를 받았는데, 그 결과...   \n",
       "\n",
       "                                                 output  \n",
       "0     위자료를 받을 수 있습니다. 사실혼은 부부간 합의 또는 부부 어느 한 쪽의 일방적인...  \n",
       "1     재판상 이혼은 재판절차를 거쳐 이혼판결이 선고된 것이므로 취소될 수 없지만, 협의이...  \n",
       "2     이혼 후 자녀를 직접 양육하지 않는 부모 일방과 자녀는 서로 만나거나 연락할 수 있...  \n",
       "3     이혼 후 자녀의 성과 본을 자신의 성과 본으로 바꿀 수 있습니다. ◇ 법원 허가 자...  \n",
       "4     유언은 의사능력이 있는 17세(유언 적령)에 달한 사람이 할 수 있습니다. 따라서 ...  \n",
       "...                                                 ...  \n",
       "2270  수의계약대상자는 견적제출자의 견적가격과 계약이행능력 등에 따라 결정되며, 원칙적으로...  \n",
       "2271  아니요. 물품계약을 체결한 날부터 90일 이상 지난 후 입찰일을 기준일로 하여 품목...  \n",
       "2272  계약을 이행하기 전이라도 일정 요건에 해당하면 계약금액의 100분의 70을 초과하지...  \n",
       "2273  계약상대자의 책임 없이 이행이 지체되는 경우에는 계약기간 연장신청을 할 수 있고, ...  \n",
       "2274  아니요. 이의신청에 따른 조치에 대하여 이의가 있으면 재심(再審)을 청구할 수 있습...  \n",
       "\n",
       "[2275 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./data/law-all-data.csv')\n",
    "df = df.drop(['input'], axis = 1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataFrameLoader(df, page_content_column=\"instruction\")\n",
    "document = loader.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = \"Salesforce/SFR-Embedding-Mistral\"\n",
    "model_name = \"jhgan/ko-sroberta-multitask\"\n",
    "model_kwargs = {'device': 'cuda'}\n",
    "encode_kwargs = {'normalize_embeddings': False}\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")\n",
    "\n",
    "cache_dir = LocalFileStore(\"./.cache/\")\n",
    "cached_embeddings = CacheBackedEmbeddings.from_bytes_store(embeddings, cache_dir)\n",
    "vectorstore = FAISS.from_documents(document, cached_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='이혼이 취소될 수 있나요?', metadata={'output': '재판상 이혼은 재판절차를 거쳐 이혼판결이 선고된 것이므로 취소될 수 없지만, 협의이혼은 부부간 자유로운 의사에 따른 합의에 기초하므로 사기 또는 강박(强迫)으로 인해 이혼의 의사표시를 했다면 이혼이 취소될 수 있습니다. 이혼을 취소하려면 관할 가정법원에 이혼취소소송을 제기해야 합니다. 이혼취소판결이 확정되면 그 이혼은 처음부터 없었던 것과 같아지므로 취소판결 전에 다른 일방이 재혼했다면 그 재혼은 중혼(重婚)이 됩니다. ◇ 중혼 ☞ 중혼은 법률상 혼인관계가 둘 이상 존재하는 위법한 상태로서 혼인의 취소사유가 됩니다. 중혼을 이유로 재혼(後婚)이 취소되면 전 배우자와만 법률상의 부부관계가 유지되고, 재혼 배우자와의 법률상 부부관계는 종료됩니다.'}),\n",
       " Document(page_content='이혼이 무효로 되는 경우가 있나요?', metadata={'output': '재판상 이혼은 재판절차를 거쳐 이혼판결이 선고된 것이므로 무효로 되지 않지만, 협의이혼은 부부 간 합의에 기초하므로 이혼에 관한 부부의 합의가 없다면 이혼이 무효로 될 수 있습니다. 예를 들어, 부부 일방 또는 쌍방이 모르는 사이에 누군가에 의해 이혼신고된 경우는 부부 사이에 이혼에 관한 합의가 없었으므로 이혼무효가 될 수 있습니다. 이혼을 무효로 하려면 관할 가정법원에 이혼무효소송을 제기해야 합니다. 이혼무효판결이 확정되면 그 이혼은 처음부터 없었던 것과 같아지므로 이전의 혼인은 중단 없이 계속된 것으로 됩니다.'}),\n",
       " Document(page_content='이혼해도 자녀를 만날 수 있나요?', metadata={'output': '이혼 후 자녀를 직접 양육하지 않는 부모 일방과 자녀는 서로 만나거나 연락할 수 있는 면접교섭권을 가집니다. 면접교섭권의 행사는 자녀의 복리를 우선적으로 고려해서 이루어져야 하므로, 자녀가 만남을 꺼려하는 등 자녀의 복리를 위해 필요한 경우에는 면접교섭이 제한되거나 배제될 수 있습니다. 상대방이 자녀와의 면접교섭을 부당하게 방해한다면 가정법원에 이행명령을 신청해서 그 의무를 이행하도록 촉구할 수 있습니다.'}),\n",
       " Document(page_content='협의이혼 절차를 진행 중인데 이혼하지 않기로 합의했어요. \\n진행 중인 협의이혼을 철회할 수 있나요?', metadata={'output': '◇ 가정법원에서 가정법원에 협의이혼의사확인 신청을 한 이후에 이혼의사가 없어졌다면 가정법원으로부터 이혼의사와 친권 양육권에 관한 사항 등을 확인받기 전까지 협의이혼의사확인 신청을 취하하면 됩니다. 부부 일방 또는 쌍방이 출석통지를 받고도 2회에 걸쳐 출석하지 않는 경우에도 협의이혼의사확인 신청을 취하한 것으로 봅니다. ◇ 시청 읍 면사무소에서 가정법원으로부터 협의이혼의사확인을 받은 이후에 이혼의사가 없어졌다면 ① 행정관청에 이혼신고서를 제출하지 않거나, ② 이혼신고서가 수리되기 전에 이혼의사확인서 등본을 첨부한 이혼의사철회서를 시청 읍 면사무소에 제출하면 이혼의사가 철회됩니다. 그러나 본인의 이혼의사철회서보다 배우자의 이혼신고서가 먼저 제출된 경우에는 이혼이 이미 성립되었기 때문에 철회서를 제출하더라도 이혼이 철회되지 않습니다.'})]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieved_docs = retriever.invoke(\"이혼이 취소될 수 있나요?\")\n",
    "retrieved_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"/home/crysis/projects/llama2\"\n",
    "peft_model_id = \"/home/crysis/projects/LLM/05_finetuning/test/03_llama2_law/wandb/llama-law-model-0206\"\n",
    "\n",
    "config = PeftConfig.from_pretrained(peft_model_id)\n",
    "bnb_confg = BitsAndBytesConfig(\n",
    "    load_in_8bit=False,\n",
    "    load_in_4bit=True,\n",
    "    llm_int8_threshold=6.0,\n",
    "    llm_int8_skip_modules=None,\n",
    "    llm_int8_enable_fp32_cpu_offload=False,\n",
    "    llm_int8_has_fp16_weight=False,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=False,\n",
    "    bnb_4bit_compute_dtype=\"float16\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:16<00:00,  8.27s/it]\n",
      "/home/crysis/anaconda3/envs/torch/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:389: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/home/crysis/anaconda3/envs/torch/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:394: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for PeftModelForCausalLM:\n\tsize mismatch for base_model.model.model.embed_tokens.weight: copying a param with shape torch.Size([32002, 4096]) from checkpoint, the shape in current model is torch.Size([32000, 4096]).\n\tsize mismatch for base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.28.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.29.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.30.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.31.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.lm_head.weight: copying a param with shape torch.Size([32002, 4096]) from checkpoint, the shape in current model is torch.Size([32000, 4096]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_id)\n\u001b[1;32m      2\u001b[0m model \u001b[38;5;241m=\u001b[39m LlamaForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_id, quantization_config\u001b[38;5;241m=\u001b[39mbnb_confg, device_map\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;241m0\u001b[39m})\n\u001b[0;32m----> 3\u001b[0m model_pt \u001b[38;5;241m=\u001b[39m \u001b[43mPeftModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpeft_model_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m pipe \u001b[38;5;241m=\u001b[39m pipeline(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext-generation\u001b[39m\u001b[38;5;124m\"\u001b[39m, model\u001b[38;5;241m=\u001b[39mmodel_pt, tokenizer\u001b[38;5;241m=\u001b[39mtokenizer, max_new_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4096\u001b[39m)\n\u001b[1;32m      5\u001b[0m hf \u001b[38;5;241m=\u001b[39m HuggingFacePipeline(pipeline\u001b[38;5;241m=\u001b[39mpipe)\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.10/site-packages/peft/peft_model.py:354\u001b[0m, in \u001b[0;36mPeftModel.from_pretrained\u001b[0;34m(cls, model, model_id, adapter_name, is_trainable, config, **kwargs)\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    353\u001b[0m     model \u001b[38;5;241m=\u001b[39m MODEL_TYPE_TO_PEFT_MODEL_MAPPING[config\u001b[38;5;241m.\u001b[39mtask_type](model, config, adapter_name)\n\u001b[0;32m--> 354\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_adapter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_trainable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_trainable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.10/site-packages/peft/peft_model.py:698\u001b[0m, in \u001b[0;36mPeftModel.load_adapter\u001b[0;34m(self, model_id, adapter_name, is_trainable, **kwargs)\u001b[0m\n\u001b[1;32m    695\u001b[0m adapters_weights \u001b[38;5;241m=\u001b[39m load_peft_weights(model_id, device\u001b[38;5;241m=\u001b[39mtorch_device, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhf_hub_download_kwargs)\n\u001b[1;32m    697\u001b[0m \u001b[38;5;66;03m# load the weights into the model\u001b[39;00m\n\u001b[0;32m--> 698\u001b[0m load_result \u001b[38;5;241m=\u001b[39m \u001b[43mset_peft_model_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapters_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madapter_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    699\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    700\u001b[0m     (\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhf_device_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    701\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mset\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhf_device_map\u001b[38;5;241m.\u001b[39mvalues())\u001b[38;5;241m.\u001b[39mintersection({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisk\u001b[39m\u001b[38;5;124m\"\u001b[39m})) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    702\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpeft_config) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    703\u001b[0m ):\n\u001b[1;32m    704\u001b[0m     device_map \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdevice_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.10/site-packages/peft/utils/save_and_load.py:222\u001b[0m, in \u001b[0;36mset_peft_model_state_dict\u001b[0;34m(model, peft_model_state_dict, adapter_name)\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m\n\u001b[0;32m--> 222\u001b[0m load_result \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpeft_model_state_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39mis_prompt_learning:\n\u001b[1;32m    224\u001b[0m     model\u001b[38;5;241m.\u001b[39mprompt_encoder[adapter_name]\u001b[38;5;241m.\u001b[39membedding\u001b[38;5;241m.\u001b[39mload_state_dict(\n\u001b[1;32m    225\u001b[0m         {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweight\u001b[39m\u001b[38;5;124m\"\u001b[39m: peft_model_state_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt_embeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m]}, strict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    226\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.10/site-packages/torch/nn/modules/module.py:2152\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2147\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2148\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2149\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[1;32m   2151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2152\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2153\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   2154\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for PeftModelForCausalLM:\n\tsize mismatch for base_model.model.model.embed_tokens.weight: copying a param with shape torch.Size([32002, 4096]) from checkpoint, the shape in current model is torch.Size([32000, 4096]).\n\tsize mismatch for base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.28.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.29.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.30.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.31.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.lm_head.weight: copying a param with shape torch.Size([32002, 4096]) from checkpoint, the shape in current model is torch.Size([32000, 4096])."
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = LlamaForCausalLM.from_pretrained(model_id, quantization_config=bnb_confg, device_map={\"\":0})\n",
    "model_pt = PeftModel.from_pretrained(model, peft_model_id)\n",
    "pipe = pipeline(\"text-generation\", model=model_pt, tokenizer=tokenizer, max_new_tokens=4096)\n",
    "hf = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Below is an instruction that describes a task. Write a response that appropriately completes the request. ### Instruction: %s ### Response: \"\n",
    "\n",
    "def gen(x):\n",
    "    q = prompt % (x,)\n",
    "    gened = model_pt.generate(\n",
    "        **tokenizer(\n",
    "            q,\n",
    "            return_tensors='pt',\n",
    "            return_token_type_ids=False\n",
    "        ).to('cuda'),\n",
    "        max_new_tokens=4096,\n",
    "        early_stopping=True,\n",
    "        do_sample=False,\n",
    "    )\n",
    "    print(q)\n",
    "    return tokenizer.decode(gened[0]).replace(q, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request. ### Instruction: 이혼 취소를 하고 싶은데 어떻게 해야해? ### Response: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<s> 이혼 취소 신청은 당사자 간 합의로 협의해서 취소 하면 됩니다. ◇ 이혼 취소의 신청 ☞ 이혼의 취소는 당사자 간의 합의로 협의해서 취소 하면 됩니다. ☞ 따라서 일방 당사자의 의사표시나 강제와 같은 불법한 방법으로 이혼 시효 중인 경우에도 다른 일방이 반대하더라도 이혼 취소를 할 수 있습니다. ☞ 그러나 한쪽 당사자가 반대하고 다른 한쪽 당사자이라도 합의가 되면 이혼 취소는 성립됩니다.</s>'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen(\"이혼 취소를 하고 싶은데 어떻게 해야해?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/crysis/anaconda3/envs/torch/lib/python3.10/site-packages/transformers/generation/utils.py:1518: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'이혼 취소의 신청은 당사자 간 이혼 취소의 합의가 있더라도, 일정 요건을 모두 갖춰야 합니다. ◇ 이혼 취소의 합의 ☞ 이혼의 취소는 당사자 간의 이혼 취소의 합의가 있더라도, ① 결혼관계가 일정기간 이상 존속되고 ② 이혼하지 않으면 당사자 간의 권리 의무 관계에 매우 큰 영향을 미치게 된다는 2가지 요건을 모두 갖추어야 합니다. ◇ 이혼 취소의 합의 시 간이혼 확인 ☞ 취소의 합의 시에 일정 기간의 유형 결혼관계가 존속한지 확인하려면 일정 기간의 유형 결혼관계를 존속하였는지를 하는 증명자료를 준비해야 합니다. ☞ 이를 위해 법원에 소장을 하여 법원이 그 사실을 확인한 후 합의의 합의 시에 그 사실을 감안할 수 있도록 해야 합니다. ◇ 이혼 취소의 합의 시 간이혼 관계행방향 확인 ☞ 취소의 합의 시에 결혼관계가 일정기간 이상 존속하였는지를 하는 증명자료를 준비할 때, 결혼 전과 이후 간 권리 변동사실 및 관계를 분석하여 그것으로 인해 결혼관계가 계속 유지되는 것으로 확인할 수 있도록 하려면 행방 형성을 감안할 수 있도록 해야 합니다.'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template = \"\"\" \n",
    "        ### Instruction: {question}\n",
    "        ### Response: \"\"\"\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "chain = prompt | hf\n",
    "\n",
    "question = \"이혼 취소를 하고 싶은데 어떻게 해야해?\"\n",
    "\n",
    "chain.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 불필요 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_col = []\n",
    "token_col = []\n",
    "encoder = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    instruction = str(row[\"instruction\"])\n",
    "    response = str(row[\"output\"])\n",
    "    \n",
    "    text = \"### Instruction:\\n\" + instruction + \"\\n### Response:\\n\" + response\n",
    "    text_col.append(text)\n",
    "    token = encoder.encode(text)\n",
    "    token_col.append(len(token))\n",
    "\n",
    "df.loc[:, \"text\"] = text_col\n",
    "df.loc[:, \"token\"] = token_col\n",
    "df.drop(['instruction', 'output'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = max(df['token'])\n",
    "t = pd.Series(df['token']).sum()\n",
    "print(f'Max Token: {m}\\nTotal Token: {t}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
